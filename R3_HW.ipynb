{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_7EvrcvfuPH"
      },
      "source": [
        "# 第三次作業"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IN-a2JhSfuPJ"
      },
      "source": [
        "## Basline\n",
        "將第一次作業的 QA 檢索聊天機器人，重新透過 LangChain 進行實作"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "57e5arDBfuPK"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "url = \"https://www.google.com/search?q=%E6%88%B4%E8%B3%87%E7%A9%8E%E5%A5%A7%E9%81%8B&sca_esv=84adf72e07b86e49&sca_upv=1&rlz=1C1VDKB_zh-TWTW1042TW1042&biw=1920&bih=919&tbm=nws&sxsrf=ADLYWIIqt57Quw-NSUjoBW48jPc5oifJMQ%3A1720616391025&ei=x4WOZvydAYHt1e8Poauo8Ak&oq=%E5%B8%B6%E5%A7%BF%E7%A9%8E&gs_lp=Egxnd3Mtd2l6LW5ld3MiCeW4tuWnv-epjioCCAEyDRAAGIAEGLEDGIMBGAoyEBAAGIAEGLEDGEMYgwEYigUyDRAAGIAEGLEDGIMBGAoyDRAAGIAEGLEDGIMBGAoyChAAGIAEGLEDGAoyDRAAGIAEGLEDGIMBGAoyDRAAGIAEGLEDGEMYigUyEBAAGIAEGLEDGIMBGIoFGAoyEBAAGIAEGLEDGIMBGIoFGAoyEBAAGIAEGLEDGIMBGIoFGApInxdQAFi1BnAAeACQAQCYATegAZ8DqgEBOLgBA8gBAPgBAZgCCKACsgPCAgUQABiABMICCBAAGIAEGKIEwgIIEAAYgAQYsQPCAgsQABiABBixAxiDAcICBBAAGAPCAg4QABiABBixAxiDARiKBcICBBAAGB7CAgYQABgFGB7CAgcQABiABBgNmAMAkgcBOKAH9RY&sclient=gws-wiz-news\"\n",
        "\n",
        "loader = WebBaseLoader(url)\n",
        "news_docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=200,\n",
        "    chunk_overlap=30)\n",
        "texts_chunks = text_splitter.split_documents(news_docs)"
      ],
      "metadata": {
        "id": "AjPF-qBgrbGs"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "# 初始化 Embedding 模型\n",
        "embedding_func = HuggingFaceEmbeddings(\n",
        "    model_name=\"infgrad/stella-base-zh-v3-1792d\",\n",
        "    model_kwargs={\"device\": \"cuda\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSzMuk_arg08",
        "outputId": "0f59e3ab-5419-4400-c0fa-ebc356371ef5"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of BertModel were not initialized from the model checkpoint at infgrad/stella-base-zh-v3-1792d and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "# load it into Chroma\n",
        "db = Chroma.from_documents(texts_chunks, embedding_func)"
      ],
      "metadata": {
        "id": "fRPrkazNmz0j"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "MODEL_NAME = \"MediaTek-Research/Breeze-7B-Instruct-v0_1\"\n",
        "\n",
        "\n",
        "# 量化參數\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True)\n",
        "\n",
        "# llm 初始化\n",
        "llm = HuggingFacePipeline.from_model_id(\n",
        "    model_id=MODEL_NAME,\n",
        "    task=\"text-generation\",\n",
        "    model_kwargs=dict(\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=quantization_config),\n",
        "    pipeline_kwargs=dict(\n",
        "        max_new_tokens=1024,\n",
        "        temperature=0.0001,\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "        repetition_penalty=1.15) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "TraXtaB0iDfZ",
        "outputId": "75e7a99a-2b69-4c7b-9770-407654236e1a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:1381: UserWarning: Current model requires 62915040.0 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-552771a489b1>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# llm 初始化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m llm = HuggingFacePipeline.from_model_id(\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_huggingface/llms/huggingface_pipeline.py\u001b[0m in \u001b[0;36mfrom_model_id\u001b[0;34m(cls, model_id, task, backend, device, device_map, model_kwargs, pipeline_kwargs, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m                         )\n\u001b[1;32m    128\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                     model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m    130\u001b[0m                         \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_model_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    564\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3702\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3703\u001b[0;31m                 \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3705\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m             }\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"disk\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m     86\u001b[0m                     \u001b[0;34m\"Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0;34m\"quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt 模板\n",
        "template = \"\"\"\n",
        "<s>\n",
        "請你做為一個羽球的專家，並根據下方所提供的資訊，來回答使用者的提問。\n",
        "\n",
        "[INST]\n",
        "{context}\n",
        "\n",
        "{question}\n",
        "[/INST]\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])"
      ],
      "metadata": {
        "id": "60XeJsLpt99U"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever(search_kwargs={\"k\": 2}),\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": prompt})\n",
        "\n",
        "result = qa_chain({\"query\": \"誰是戴資穎\"})\n",
        "\n",
        "# LLM 提供的回答\n",
        "print(\"回覆：\")\n",
        "print(result[\"result\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4soeIuzjdf4",
        "outputId": "6d48675c-42fe-462f-869b-987279866484"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "回覆：\n",
            "\n",
            "<s>\n",
            "請你做為一個羽球的專家，並根據下方所提供的資訊，來回答使用者的提問。\n",
            "\n",
            "[INST]\n",
            "運動中央社 CNA網球名將謝淑薇退賽與「羽球天后」戴資穎的球鞋爭議，讓2016里約奧運賽場外話題搶走不少運動員的風采，當年靠舉重好手許淑淨奪金帶領，台灣代表團共拿...6 hours ago羽球》戴資穎奧運最後一舞表現受關注 教練賴建誠透露目前狀態自由體育台灣羽球一姐戴資穎今天出席國訓中心的巴黎奧運授旗典禮，雖然她低調婉拒媒體訪問，仍大方和其他選手及與會來賓合照。由於小戴之前有傷在身，...1\n",
            "\n",
            "戴資穎奧運 - Google SearchGoogle×Please click here if you are not redirected within a few seconds.    AllNewsVideosImages Maps Shopping Books Search tools    RecentRecentPast hourPast 24 hoursPast weekPast\n",
            "\n",
            "誰是戴資穎\n",
            "[/INST]\n",
            "戴資穎是一名來自臺灣的職業羽毛球選手，她是臺灣的女單世界排名第一的球員。她在2016年里約奧運會中獲得銅牌獎牌，並在2021年東京奧運會上奪得銀牌。她被譽為臺灣的「羽球后」，也是臺灣體壇界的重要人物之一。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIu0B4ZvfuPK"
      },
      "source": [
        "## Advance\n",
        "延伸 Basline 的內容，測試看看不同的 chunk 大小，對於生成結果的影響"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "PRw3rQKcfuPL"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=30)\n",
        "texts_chunks = text_splitter.split_documents(news_docs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "# load it into Chroma\n",
        "db = Chroma.from_documents(texts_chunks, embedding_func)"
      ],
      "metadata": {
        "id": "zn0o0kzUvEHk"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever(search_kwargs={\"k\": 2}),\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": prompt})\n",
        "\n",
        "result = qa_chain({\"query\": \"誰是戴資穎\"})\n",
        "\n",
        "# LLM 提供的回答\n",
        "print(\"回覆：\")\n",
        "print(result[\"result\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3S_4Qhl-vJqR",
        "outputId": "9be9b2d9-da54-4190-fa4c-ab4d36b367e3"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "回覆：\n",
            "\n",
            "<s>\n",
            "請你做為一個羽球的專家，並根據下方所提供的資訊，來回答使用者的提問。\n",
            "\n",
            "[INST]\n",
            "運動中央社 CNA網球名將謝淑薇退賽與「羽球天后」戴資穎的球鞋爭議，讓2016里約奧運賽場外話題搶走不少運動員的風采，當年靠舉重好手許淑淨奪金帶領，台灣代表團共拿...6 hours ago羽球》戴資穎奧運最後一舞表現受關注 教練賴建誠透露目前狀態自由體育台灣羽球一姐戴資穎今天出席國訓中心的巴黎奧運授旗典禮，雖然她低調婉拒媒體訪問，仍大方和其他選手及與會來賓合照。由於小戴之前有傷在身，...1\n",
            "\n",
            "戴資穎確定第3種子進軍巴黎奧運自由體育超級500系列加拿大公開賽落幕後，羽球媒體《Statminton》公布本週最新世界排名，台灣一姊戴資穎持續高居女單第3。戴資穎的積分92221分維持不變，...\n",
            "\n",
            "誰是戴資穎\n",
            "[/INST]\n",
            "戴資穎是一名來自臺灣的職業羽毛球選手，她是臺灣的國家隊成員。她在女子單打項目中獲得了許多成功，包括在國際賽事上贏得多個冠軍頭銜。她被譽為臺灣的「羽球后」，也是臺灣體壇界的重要人物之一。\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}